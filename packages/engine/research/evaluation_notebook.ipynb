{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_methods import EvaluationMethods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_methods = EvaluationMethods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does the ELO Rating System work in the context of prompt testing and ranking?: 1588.4422577440132\n",
      "Explain the importance of Automatic Evaluation Data Generation in PromptlyTech's offerings.: 1543.6798996529228\n",
      "What are some innovative approaches suggested for prompt evaluation in Task 5?: 1532.3104658608177\n",
      "How does the user interface contribute to the overall prompt engineering system in Task 5?: 1525.6051803007426\n",
      "What is the primary business objective of PromptlyTech?: 1515.1359449372885\n",
      "Name the key services provided by PromptlyTech.: 1508.719376595432\n"
     ]
    }
   ],
   "source": [
    "# Define prompts and initial ratings\n",
    "prompts = [\n",
    "    \"What is the primary business objective of PromptlyTech?\",\n",
    "    \"Name the key services provided by PromptlyTech.\",\n",
    "    \"Explain the importance of Automatic Evaluation Data Generation in PromptlyTech's offerings.\",\n",
    "    \"How does the ELO Rating System work in the context of prompt testing and ranking?\",\n",
    "    \"What are some innovative approaches suggested for prompt evaluation in Task 5?\",\n",
    "    \"How does the user interface contribute to the overall prompt engineering system in Task 5?\"\n",
    "]\n",
    "elo_ratings = {prompt: 1500 for prompt in prompts}  # Initial ratings\n",
    "\n",
    "# Conduct multiple rounds of evaluation\n",
    "for _ in range(10):  # Number of rounds\n",
    "    elo_ratings = eval_methods.elo_ratings_func(prompts, elo_ratings)\n",
    "\n",
    "# Sort prompts by their final Elo ratings\n",
    "sorted_prompts = sorted(prompts, key=lambda x: elo_ratings[x], reverse=True)\n",
    "\n",
    "# Print the ranked prompts\n",
    "for prompt in sorted_prompts:\n",
    "    print(f\"{prompt}: {elo_ratings[prompt]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts evaluation\n",
    "1. **How does the user interface contribute to the overall prompt engineering system in Task 5?:** 1557.24\n",
    "   - This prompt has a **high rating**, suggesting it was evaluated as very relevant and valuable to understanding the contribution of the user interface to the prompt engineering system in Task 5.\n",
    "\n",
    "2. **What are some innovative approaches suggested for prompt evaluation in Task 5?:** 1543.87\n",
    "   - This prompt also received a **high rating**, indicating that it was considered valuable in exploring innovative approaches for prompt evaluation in Task 5.\n",
    "\n",
    "3. **What is the primary business objective of PromptlyTech?:** 1528.40\n",
    "   - The prompt received a **solid rating**, indicating it was perceived as relevant and important in understanding the primary business objective of PromptlyTech.\n",
    "\n",
    "4. **How does the ELO Rating System work in the context of prompt testing and ranking?:** 1511.07\n",
    "   - This prompt has a **good rating**, suggesting it was seen as valuable in explaining the functioning of the ELO Rating System in the context of prompt testing and ranking.\n",
    "\n",
    "5. **Name the key services provided by PromptlyTech.:** 1508.48\n",
    "   - The prompt received a **reasonable rating**, indicating it was considered important in identifying the key services provided by PromptlyTech.\n",
    "\n",
    "6. **Explain the importance of Automatic Evaluation Data Generation in PromptlyTech's offerings.:** 1497.73\n",
    "   - This prompt received a **slightly lower rating**, suggesting it may be perceived as less critical compared to other prompts in understanding the importance of Automatic Evaluation Data Generation in PromptlyTech's offerings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompt(main_prompt, test_cases):\n",
    "    evaluations = {}\n",
    "\n",
    "    # Evaluate the main prompt using Monte Carlo and Elo methods\n",
    "    evaluations['main_prompt'] = {\n",
    "        'Monte Carlo Evaluation': eval_methods.monte_carlo_eval(main_prompt),\n",
    "        'Elo Rating Evaluation': eval_methods.elo_eval(main_prompt)\n",
    "    }\n",
    "\n",
    "    # Evaluate each test case\n",
    "    for idx, test_case in enumerate(test_cases):\n",
    "        evaluations[f'test_case_{idx+1}'] = {\n",
    "            'Monte Carlo Evaluation': eval_methods.monte_carlo_eval(test_case),\n",
    "            'Elo Rating Evaluation': eval_methods.elo_eval(test_case)\n",
    "        }\n",
    "\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'main_prompt': {'Monte Carlo Evaluation': 1.9, 'Elo Rating Evaluation': 1504.2019499940866}, 'test_case_1': {'Monte Carlo Evaluation': 1.97, 'Elo Rating Evaluation': 1489.2019499940866}, 'test_case_2': {'Monte Carlo Evaluation': 1.98, 'Elo Rating Evaluation': 1489.2019499940866}, 'test_case_3': {'Monte Carlo Evaluation': 2.02, 'Elo Rating Evaluation': 1504.2019499940866}, 'test_case_4': {'Monte Carlo Evaluation': 2.04, 'Elo Rating Evaluation': 1489.2019499940866}, 'test_case_5': {'Monte Carlo Evaluation': 2.07, 'Elo Rating Evaluation': 1504.2019499940866}, 'test_case_6': {'Monte Carlo Evaluation': 1.96, 'Elo Rating Evaluation': 1489.2019499940866}}\n"
     ]
    }
   ],
   "source": [
    "main_prompt = \"How does effective prompt engineering contribute to the success of AI-driven solutions, especially in optimizing the use of Language Models (LLMs) in various industries?\"\n",
    "test_cases = [\n",
    "    \"What role does prompt engineering play in enhancing decision-making, operational efficiency, and customer experience in various industries?\",\n",
    "    \"How does Automatic Prompt Generation by PromptlyTech streamline the process of creating effective prompts for businesses?\",\n",
    "    \"In what ways does Automatic Evaluation Data Generation by PromptlyTech contribute to enhancing the reliability and performance of LLM applications?\",\n",
    "    \"Explain the significance of PromptlyTech's Prompt Testing and Ranking Service in ensuring accurate and contextually relevant responses from chatbots and virtual assistants.\",\n",
    "    \"Can you elaborate on the innovative approaches mentioned for prompt evaluation in Task 5 of the challenge?\",\n",
    "    \"How does the user interface developed for prompt engineering contribute to the overall user experience in Task 5?\"\n",
    "]\n",
    "result = evaluate_prompt(main_prompt, test_cases)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "#### 1. Monte Carlo Evaluation:\n",
    "   - **Scores Range:** From 1 to 3, with higher scores indicating greater relevance or quality of the prompt.\n",
    "   ###### Interpretation:\n",
    "   - **1.9 (Main Prompt):** Slightly below average relevance or quality.\n",
    "   - **1.97, 1.98, 2.02, 2.04, 2.07 (Test Cases):** Scores around 2 suggest moderate relevance or quality. The variation indicates some test cases are deemed slightly more relevant or higher quality than others.\n",
    "\n",
    "#### 2. Elo Rating Evaluation:\n",
    "   - **Base Rating:** Usually starts at 1500, with changes based on the 'performance' of the prompt against a set of standards.\n",
    "   - **Higher than 1500:** Indicates the prompt performed better than average.\n",
    "   - **Lower than 1500:** Indicates the prompt performed worse than average.\n",
    "   ###### Interpretation:\n",
    "   - **1504.20 (Main Prompt):** Slightly below the average performance.\n",
    "   - **1489.20 (Test Cases 1, 2, 4, 5):** These prompts are rated above the average, suggesting better performance.\n",
    "   - **1504.20 (Test Case 3):** Slightly above average performance.\n",
    "\n",
    "#### Overall Interpretation:\n",
    "   - **Main Prompt:** Both evaluations suggest that the main prompt is slightly below average in terms of relevance and quality.\n",
    "   - **Test Cases:** Generally, the test cases are rated as average or slightly above average in both relevance and quality. Test Cases 1, 2, 4, and 5 seem to perform particularly well in the Elo evaluation, indicating they might be more effective or well-structured prompts compared to the main prompt and Test Case 3.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter  \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "import os\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "# \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "def data_loader(file_path= '../prompts/context.txt'):\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Chunk the data\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=15773, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(chunks):\n",
    "    load_dotenv(find_dotenv())\n",
    "    \n",
    "\n",
    "    # # Setup vector database\n",
    "    client = weaviate.Client(\n",
    "    embedded_options = EmbeddedOptions()\n",
    "    )\n",
    "    \n",
    "    # Populate vector database\n",
    "    vectorstore = Weaviate.from_documents(\n",
    "      client = client,    \n",
    "      documents = chunks,\n",
    "      embedding = OpenAIEmbeddings(),\n",
    "      by_text = False\n",
    "    )\n",
    "    \n",
    "    # Define vectorstore as retriever to enable semantic search\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started /Users/azizamed/.cache/weaviate-embedded: process ID 37482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"startup\",\"default_vectorizer_module\":\"none\",\"level\":\"info\",\"msg\":\"the default vectorizer modules is set to \\\"none\\\", as a result all new schema classes without an explicit vectorizer setting, will use this vectorizer\",\"time\":\"2024-01-19T02:53:06+03:00\"}\n",
      "{\"action\":\"startup\",\"auto_schema_enabled\":true,\"level\":\"info\",\"msg\":\"auto schema enabled setting is set to \\\"true\\\"\",\"time\":\"2024-01-19T02:53:06+03:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"No resource limits set, weaviate will use all available memory and CPU. To limit resources, set LIMIT_RESOURCES=true\",\"time\":\"2024-01-19T02:53:06+03:00\"}\n",
      "{\"level\":\"warning\",\"msg\":\"Multiple vector spaces are present, GraphQL Explore and REST API list objects endpoint module include params has been disabled as a result.\",\"time\":\"2024-01-19T02:53:06+03:00\"}\n",
      "{\"action\":\"grpc_startup\",\"level\":\"info\",\"msg\":\"grpc server listening at [::]:50060\",\"time\":\"2024-01-19T02:53:06+03:00\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Serving weaviate at http://127.0.0.1:8079\",\"time\":\"2024-01-19T02:53:06+03:00\"}\n",
      "{\"action\":\"read_disk_use\",\"level\":\"warning\",\"msg\":\"disk usage currently at 91.91%, threshold set to 80.00%\",\"path\":\"/Users/azizamed/.local/share/weaviate\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_17bbdaa4279c488db6082b6609d17e65_Wo3w0e0EQ5AM in 14.079152ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_17bbdaa4279c488db6082b6609d17e65\",\"index\":\"langchain_17bbdaa4279c488db6082b6609d17e65\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_17bbdaa4279c488db6082b6609d17e65/Wo3w0e0EQ5AM/lsm\",\"shard\":\"Wo3w0e0EQ5AM\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":129836}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_361d7bd500a746eb9326b8c2c116d088_gyJH0P1pTUrf in 2.609059ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_361d7bd500a746eb9326b8c2c116d088\",\"index\":\"langchain_361d7bd500a746eb9326b8c2c116d088\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_361d7bd500a746eb9326b8c2c116d088/gyJH0P1pTUrf/lsm\",\"shard\":\"gyJH0P1pTUrf\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":72086}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_7e6b06461f0c41709a31b462433baa0d_LPIrG0fErBiY in 2.795823ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_7e6b06461f0c41709a31b462433baa0d\",\"index\":\"langchain_7e6b06461f0c41709a31b462433baa0d\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_7e6b06461f0c41709a31b462433baa0d/LPIrG0fErBiY/lsm\",\"shard\":\"LPIrG0fErBiY\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":79519}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_879b583a0e644a58b62b7e37ebe2a3b0_b8dmtMmmcU2a in 2.623922ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_879b583a0e644a58b62b7e37ebe2a3b0\",\"index\":\"langchain_879b583a0e644a58b62b7e37ebe2a3b0\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_879b583a0e644a58b62b7e37ebe2a3b0/b8dmtMmmcU2a/lsm\",\"shard\":\"b8dmtMmmcU2a\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":71641}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_bd94deabf3fd450493a14695b0293bc8_MgthMbooTVoq in 2.678914ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_bd94deabf3fd450493a14695b0293bc8\",\"index\":\"langchain_bd94deabf3fd450493a14695b0293bc8\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_bd94deabf3fd450493a14695b0293bc8/MgthMbooTVoq/lsm\",\"shard\":\"MgthMbooTVoq\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":74451}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_f1903ccca9934092999f9b13ae383d2b_WXzB6bLwg8BL in 3.792699ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_f1903ccca9934092999f9b13ae383d2b\",\"index\":\"langchain_f1903ccca9934092999f9b13ae383d2b\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_f1903ccca9934092999f9b13ae383d2b/WXzB6bLwg8BL/lsm\",\"shard\":\"WXzB6bLwg8BL\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":78891}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_3c5eaa7ac2e64aa78088115afe2d764f_BI04w1Ja6MAn in 2.183649ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_3c5eaa7ac2e64aa78088115afe2d764f\",\"index\":\"langchain_3c5eaa7ac2e64aa78088115afe2d764f\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_3c5eaa7ac2e64aa78088115afe2d764f/BI04w1Ja6MAn/lsm\",\"shard\":\"BI04w1Ja6MAn\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":78767}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_619c6b40837e43d784a7afe3ddf243d2_glTRueqVIBtl in 6.898213ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_619c6b40837e43d784a7afe3ddf243d2\",\"index\":\"langchain_619c6b40837e43d784a7afe3ddf243d2\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_619c6b40837e43d784a7afe3ddf243d2/glTRueqVIBtl/lsm\",\"shard\":\"glTRueqVIBtl\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":453260}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_be0621772dc242e19f5804db2bab9a12_OsuEejcHErWq in 2.503939ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_be0621772dc242e19f5804db2bab9a12\",\"index\":\"langchain_be0621772dc242e19f5804db2bab9a12\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_be0621772dc242e19f5804db2bab9a12/OsuEejcHErWq/lsm\",\"shard\":\"OsuEejcHErWq\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":89718}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_caab8639092a4993a3f5812d0527f236_EKYHpwF3ViXn in 2.895485ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_caab8639092a4993a3f5812d0527f236\",\"index\":\"langchain_caab8639092a4993a3f5812d0527f236\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_caab8639092a4993a3f5812d0527f236/EKYHpwF3ViXn/lsm\",\"shard\":\"EKYHpwF3ViXn\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":80351}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_0841ccc6e48b4569943d2dd8f0900380_fOhxAQ0mA7xl in 2.460302ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_0841ccc6e48b4569943d2dd8f0900380\",\"index\":\"langchain_0841ccc6e48b4569943d2dd8f0900380\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_0841ccc6e48b4569943d2dd8f0900380/fOhxAQ0mA7xl/lsm\",\"shard\":\"fOhxAQ0mA7xl\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":77517}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_19fc547c894944deb61033ce03a35df0_wITsEGidABEF in 2.136102ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_19fc547c894944deb61033ce03a35df0\",\"index\":\"langchain_19fc547c894944deb61033ce03a35df0\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_19fc547c894944deb61033ce03a35df0/wITsEGidABEF/lsm\",\"shard\":\"wITsEGidABEF\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":77788}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_8926263446bf4bb497bea99f0143898c_7bSQWRHhp7a5 in 1.886676ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_8926263446bf4bb497bea99f0143898c\",\"index\":\"langchain_8926263446bf4bb497bea99f0143898c\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_8926263446bf4bb497bea99f0143898c/7bSQWRHhp7a5/lsm\",\"shard\":\"7bSQWRHhp7a5\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":81397}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_a4d3bb5037f74c1cbb5ddbda4a30ad63_dfBgdkLswFYI in 3.06607ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_a4d3bb5037f74c1cbb5ddbda4a30ad63\",\"index\":\"langchain_a4d3bb5037f74c1cbb5ddbda4a30ad63\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_a4d3bb5037f74c1cbb5ddbda4a30ad63/dfBgdkLswFYI/lsm\",\"shard\":\"dfBgdkLswFYI\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":77726}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_bb4d98ea2fe84e1998373df882807506_rjL6yJUQEV3j in 2.384561ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_bb4d98ea2fe84e1998373df882807506\",\"index\":\"langchain_bb4d98ea2fe84e1998373df882807506\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_bb4d98ea2fe84e1998373df882807506/rjL6yJUQEV3j/lsm\",\"shard\":\"rjL6yJUQEV3j\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":71496}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_254b43779e744f9886521bc56fe591f6_0iY927OAD4uU in 2.768701ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_254b43779e744f9886521bc56fe591f6\",\"index\":\"langchain_254b43779e744f9886521bc56fe591f6\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_254b43779e744f9886521bc56fe591f6/0iY927OAD4uU/lsm\",\"shard\":\"0iY927OAD4uU\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":89510}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_274b8b31463e45ccb35ca23ebd3123b5_AQSjQDG1uhmh in 1.959798ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_274b8b31463e45ccb35ca23ebd3123b5\",\"index\":\"langchain_274b8b31463e45ccb35ca23ebd3123b5\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_274b8b31463e45ccb35ca23ebd3123b5/AQSjQDG1uhmh/lsm\",\"shard\":\"AQSjQDG1uhmh\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":104542}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_2d9294fc0e3448088208bbe039352c90_Oq2zV98SIhsP in 2.464321ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_2d9294fc0e3448088208bbe039352c90\",\"index\":\"langchain_2d9294fc0e3448088208bbe039352c90\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_2d9294fc0e3448088208bbe039352c90/Oq2zV98SIhsP/lsm\",\"shard\":\"Oq2zV98SIhsP\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":82327}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_6c31f46e683c4955840a133107b0e8e2_ZWI1NxXy3WNV in 1.788302ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_6c31f46e683c4955840a133107b0e8e2\",\"index\":\"langchain_6c31f46e683c4955840a133107b0e8e2\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_6c31f46e683c4955840a133107b0e8e2/ZWI1NxXy3WNV/lsm\",\"shard\":\"ZWI1NxXy3WNV\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":75303}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_97b8f81b7d0b4287ac653954803ac4b4_ruzjwI6fKcs6 in 5.598087ms\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"lsm_compaction\",\"class\":\"LangChain_97b8f81b7d0b4287ac653954803ac4b4\",\"index\":\"langchain_97b8f81b7d0b4287ac653954803ac4b4\",\"level\":\"warning\",\"msg\":\"compaction halted due to shard READONLY status\",\"path\":\"/Users/azizamed/.local/share/weaviate/langchain_97b8f81b7d0b4287ac653954803ac4b4/ruzjwI6fKcs6/lsm\",\"shard\":\"ruzjwI6fKcs6\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"set_shard_read_only\",\"level\":\"warning\",\"msg\":\"Set READONLY, disk usage currently at 91.91%, threshold set to 90.00%\",\"path\":\"/Users/azizamed/.local/share/weaviate\",\"time\":\"2024-01-19T02:53:07+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:07+03:00\",\"took\":162184}\n",
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n",
      "{\"level\":\"info\",\"msg\":\"Created shard langchain_42d8c525334d4bf2aea0cd6fc8258e95_xvoxSUdvesEz in 2.844828ms\",\"time\":\"2024-01-19T02:53:08+03:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-01-19T02:53:08+03:00\",\"took\":106767}\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-HgabC***************************************N2zH. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m chunks \u001b[38;5;241m=\u001b[39m  data_loader()\n\u001b[0;32m----> 2\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mcreate_retriever\u001b[0;34m(chunks)\u001b[0m\n\u001b[1;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m weaviate\u001b[38;5;241m.\u001b[39mClient(\n\u001b[1;32m      7\u001b[0m embedded_options \u001b[38;5;241m=\u001b[39m EmbeddedOptions()\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Populate vector database\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mWeaviate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43mby_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Define vectorstore as retriever to enable semantic search\u001b[39;00m\n\u001b[1;32m     19\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_core/vectorstores.py:508\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    507\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_community/vectorstores/weaviate.py:468\u001b[0m, in \u001b[0;36mWeaviate.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, client, weaviate_url, weaviate_api_key, batch_size, index_name, text_key, by_text, relevance_score_fn, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m client\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mexists(index_name):\n\u001b[1;32m    466\u001b[0m     client\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mcreate_class(schema)\n\u001b[0;32m--> 468\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m embedding \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    469\u001b[0m attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(metadatas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;28;01mif\u001b[39;00m metadatas \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# If the UUID of one of the objects already exists\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# then the existing object will be replaced by the new object.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_community/embeddings/openai.py:668\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    667\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_community/embeddings/openai.py:494\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    492\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 494\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43membed_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    500\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_community/embeddings/openai.py:116\u001b[0m, in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/resources/embeddings.py:108\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    103\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1167\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1155\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1163\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1164\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1165\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1166\u001b[0m     )\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:856\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    849\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    854\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    855\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:947\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    944\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    946\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    950\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    951\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    954\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    955\u001b[0m )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-HgabC***************************************N2zH. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "chunks =  data_loader()\n",
    "retriever = create_retriever(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(template)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Setup RAG pipeline\u001b[39;00m\n\u001b[1;32m     17\u001b[0m rag_chain \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 18\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mretriever\u001b[49m,  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: RunnablePassthrough()} \n\u001b[1;32m     19\u001b[0m     \u001b[38;5;241m|\u001b[39m prompt \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;241m|\u001b[39m llm\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser() \n\u001b[1;32m     22\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-1106-preview\", temperature=0)\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use two sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Setup RAG pipeline\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_community/embeddings/openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/usr/local/lib/python3.11/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "/usr/local/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:458: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/usr/local/lib/python3.11/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "/usr/local/lib/python3.11/site-packages/langchain_community/embeddings/openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/usr/local/lib/python3.11/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "/usr/local/lib/python3.11/site-packages/langchain_community/embeddings/openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/usr/local/lib/python3.11/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "/usr/local/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:458: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/usr/local/lib/python3.11/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "/usr/local/lib/python3.11/site-packages/langchain_community/embeddings/openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/usr/local/lib/python3.11/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "/usr/local/lib/python3.11/site-packages/langchain_community/embeddings/openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/usr/local/lib/python3.11/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "/usr/local/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:458: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/usr/local/lib/python3.11/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "/usr/local/lib/python3.11/site-packages/langchain_community/embeddings/openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/usr/local/lib/python3.11/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions = [\n",
    "    \"What is the primary business objective of PromptlyTech?\",\n",
    "    \"Name the key services provided by PromptlyTech.\",\n",
    "    \"Explain the importance of Automatic Evaluation Data Generation in PromptlyTech's offerings.\",\n",
    "    \"How does the ELO Rating System work in the context of prompt testing and ranking?\",\n",
    "    \"What are some innovative approaches suggested for prompt evaluation in Task 5?\",\n",
    "    \"How does the user interface contribute to the overall prompt engineering system in Task 5?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    [\"PromptlyTech is an innovative e-business specializing in providing AI-driven solutions for optimizing the use of Language Models (LLMs) in various industries. The company aims to revolutionize how businesses interact with LLMs, making the technology more accessible, efficient, and effective. By addressing the challenges of prompt engineering, the company plays a pivotal role in enhancing decision-making, operational efficiency, and customer experience across various industries.\"],\n",
    "    [\"PromptlyTech focuses on key services: Automatic Prompt Generation, Automatic Evaluation Data Generation, and Prompt Testing and Ranking.\"],\n",
    "    [\"Automatic Evaluation Data Generation is a crucial service offered by PromptlyTech. This service automates the generation of diverse test cases, ensuring comprehensive coverage and identifying potential issues. By creating a set of test cases that serve as evaluation benchmarks for prompt candidates, PromptlyTech enhances the reliability and performance of LLM applications. This, in turn, saves significant time in the Quality Assurance (QA) process.\"],\n",
    "    [\"The ELO Rating System, commonly used in chess and other competitive games, rates prompts based on their performance in battles. Each prompt candidate is assigned a rating that reflects its success in previous matchups. The system takes into account not just the number of wins but also the strength of the opponents each prompt has defeated. This rating helps in objectively ranking the prompts based on their effectiveness in generating desired outcomes.\"],\n",
    "    [\"Task 5 emphasizes adopting innovative approaches to prompt evaluation, including utilizing Monte Carlo matchmaking and ELO rating systems. Additionally, alternative methods such as TrueSkill Rating System, Glicko Rating System, Bayesian Rating Systems, Pairwise Comparison Methods, Categorical Ranking, Adaptive Ranking Algorithms, and Semantic Similarity Matching are mentioned. These methods provide a dynamic and adaptive framework for evaluating prompts in various contexts.\"],\n",
    "    [\"The user interface plays a crucial role in Task 5 by providing a user-friendly platform for interacting with the prompt engineering system. It allows users to easily input data, receive prompts, and view evaluation results. The design and implementation of the user interface aim to enhance the overall user experience, making it intuitive and efficient for users to engage with the automated prompt generation, evaluation data generation, and prompt testing components.\"]\n",
    "]\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "# Inference\n",
    "for query in questions:\n",
    "\n",
    "  answers.append(rag_chain.invoke(query))\n",
    "  contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
    "\n",
    "# To dict\n",
    "data = {\n",
    "    \"question\": questions, # list \n",
    "    \"answer\": answers, # list\n",
    "    \"contexts\": contexts, # list list\n",
    "    \"ground_truths\": ground_truths # list Lists\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_precision]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [context_recall]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 1/1 [00:14<00:00, 14.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/site-packages/langchain_community/embeddings/openai.py:500: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  response = response.dict()\n",
      "/usr/local/lib/python3.11/site-packages/pydantic/main.py:979: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', DeprecationWarning)\n",
      "100%|██████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.72s/it]\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    ")\n",
    "\n",
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who founded OpenAI?</td>\n",
       "      <td>OpenAI was founded by Sam Altman, Elon Musk, I...</td>\n",
       "      <td>[OpenAI was initially founded in 2015 by Sam A...</td>\n",
       "      <td>[Sam Altman, Elon Musk, Ilya Sutskever and Gre...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What was the initial goal of OpenAI?</td>\n",
       "      <td>The initial goal of OpenAI was to advance digi...</td>\n",
       "      <td>[OpenAI was initially founded in 2015 by Sam A...</td>\n",
       "      <td>[To advance digital intelligence in a way that...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What did OpenAI release in 2016?</td>\n",
       "      <td>OpenAI released 'OpenAI Gym' in 2016, a toolki...</td>\n",
       "      <td>[The early years of OpenAI were marked with ra...</td>\n",
       "      <td>[OpenAI Gym, a toolkit for developing and comp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.899221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               question  \\\n",
       "0                   Who founded OpenAI?   \n",
       "1  What was the initial goal of OpenAI?   \n",
       "2      What did OpenAI release in 2016?   \n",
       "\n",
       "                                              answer  \\\n",
       "0  OpenAI was founded by Sam Altman, Elon Musk, I...   \n",
       "1  The initial goal of OpenAI was to advance digi...   \n",
       "2  OpenAI released 'OpenAI Gym' in 2016, a toolki...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [OpenAI was initially founded in 2015 by Sam A...   \n",
       "1  [OpenAI was initially founded in 2015 by Sam A...   \n",
       "2  [The early years of OpenAI were marked with ra...   \n",
       "\n",
       "                                       ground_truths  context_precision  \\\n",
       "0  [Sam Altman, Elon Musk, Ilya Sutskever and Gre...                1.0   \n",
       "1  [To advance digital intelligence in a way that...                1.0   \n",
       "2  [OpenAI Gym, a toolkit for developing and comp...                1.0   \n",
       "\n",
       "   context_recall  faithfulness  answer_relevancy  \n",
       "0             1.0           1.0          0.959185  \n",
       "1             1.0           1.0          0.999999  \n",
       "2             1.0           1.0          0.899221  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integration with Retrieval-Augmented Generation Assessment:\n",
    "##### Monte Carlo for Robustness Testing: Use Monte Carlo simulations to test the robustness of the RAG system across a wide range of possible retrieval scenarios. This helps in understanding how different types of retrieved information can impact the quality of the generated content.\n",
    "##### Elo Rating for Continuous Improvement: Utilize the Elo rating system to continuously assess and improve the RAG model. By comparing new outputs with previous ones and adjusting ratings accordingly, the system can learn which types of retrieval-augmented generations work best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7f3c0b24c41e80e94b84b01069c679ceb9a0604be81ab76bb66c9ea948f7d76b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
